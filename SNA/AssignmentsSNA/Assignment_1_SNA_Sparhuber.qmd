---
title: "Assignment 1 SNA"
author: "Marc Sparhuber"
format: pdf
editor: source
execute:
  warning: false
  echo: false
toc: true
header-includes:
      - \usepackage{float}
      - \floatplacement{table}{H}
---

## Task 1 & Packages

Write an R-script that reads your data. Transform your data to a data frame. Argue about the format you have chosen to analyze large network data.

```{r}

# loading in all necessary packages

library(tidyverse)
library(igraph)
library(sna)
library(knitr)
library(kableExtra)
library(scales)
library(modelsummary)

# here I load in the data. To not have any issues with how the .txt file is structured I remove everything but the data and add the column names manually

data <- as.data.frame(read_table(file = "Email-Enron.txt",
                   skip = 4,
                   col_names = c("FromNodeID","ToNodeID")))
```

> The data conveniently comes in an edge list format and will be transformed to an igraph graph object later on after having been imported as a data frame. The data comes from the Enron email network which has been used for publications and in court. Due to having no other variables aside from the nodes and ties available, I will treat it like just any email network from a large corporation.

## Task 2

Make sure that before or while doing any calculations or plots that you handle nonresponse / missing data meaningfully if there is any.

```{r}
#| eval: false

# check if there are any NAs in the data

any(is.na(data))
```

> In neither of the two columns any missing data is present!

## Task 3

Justify what you do with isolates and multiple components if there are any.

```{r}
# this creates an undirected igraph graph object from the data frame
graph <- graph_from_data_frame(data, directed = FALSE)

# calucate density
density <- edge_density(graph)

# due to this being an undirected network, it makes no sense to caluclate in/outdegree. Instead, only the degrees are caluclated.
degrees <- igraph::degree(graph, mode='all')
mean_degrees <- mean(degrees)
median_degrees <- median(degrees)

# calucate standard deviation of degrees
sd_degrees <- sd(degrees)

# is 1 because all ties are undirected and no ties go beyond the borders of the network
reciprocity <- reciprocity(graph)

# calucate transitivity
transitivity <- transitivity(graph)
```

```{r}
#| eval: false

# are there any zeroes in degrees? This would indicate an isolate
0 %in% degrees
```

```{r}
# check how many components there are
n_components <- igraph::components(graph)$no

# turn output of components into data frame for visualization
data_components <- as.data.frame(igraph::components(graph)$csize)
```

```{r}
# set the theme for ggplot
theme_set(theme_light())

# create bar plot of components
data_components |> ggplot(aes(`igraph::components(graph)$csize`)) +
  geom_bar() +
  xlab("Components") +
  ylab("Count") +
  scale_x_continuous(limits = c(1, quantile(data_components$`igraph::components(graph)$csize`, 0.99)), breaks = c(1:10)) +
  labs(title = "Distribution of the lower 99th percentile of components",
       caption = "The mode is 2 and the max 33696.")
```

> This data does not contain any isolates, as the data supplied contains no NAs and came in an edge list format. To double check, a graph object in igraph and its degrees are computed and then tested on whether there are any 0's in the degrees, as a node having 0 degrees would indicate it being an isolate.

> Within the data, many components emerge. In total, there are 1065 components in this data. Their distribution can be observed in the above bar graph. As seems reasonable within the email network of a large corporation, there is one huge component, which makes up the vast majority of nodes. There are, however also many small components.

> For this analysis keeping the smaller components does not make much sense. Most ties (33696 of 36692) are part of the largest component and the majority of them containing very little information due to them only being 2-5 vertices large as well as keeping visual clutter (which is horrendous and seemingly unavoidable in this analysis) in mind, only the largest component will be taken into account in some visualizations but the entire network will always be used when calculating scores, etc.

## Task 4

Analyze the density of your network. Create a table that contains further descriptive network statistics for your network. Please include average degree (in-degree and outdegree), standard deviation of degree (in-degrees and out-degrees), reciprocity, and transitivity.

```{r}
# combining these measures into a table
kable(tribble(~Density, ~MeanDegrees, ~MedianDegrees, ~SDDegrees, ~Reciprocity, ~Transitivity,
        density, mean_degrees, median_degrees, sd_degrees, reciprocity, transitivity), caption = "Network Descriptives") |> kable_styling() |> 
   add_footnote("Due to this network being undirected, in- and out-degree are not calculated", notation = "alphabet")
```

> The very low density in this network implies that only very few of the possible number of edges in the graph actually exist. This is likely influenced by the many components but is likely mainly due to the way large corporations work - with a clear hierarchy disallowing "just any" employee to message the CEO.

> Looking at degree descriptives it becomes apparent that the degrees are highly right-skewed with the mean far from the median and a very high standard deviation indicating the great variability in the data. Thus, a majority of the individuals have very low values and looking at the degree distribution in Task 5 (not in the visualization) we can see that there is a long tail toward the higher values. In- and outdegrees were not calculated as these would have been identical in this data set.

> Recipriocity is 1, due to all ties being undirected and no ties going beyond the borders of the network. The transitivity is fairly low, indicating low levels of clustering within the network. It is important to note that this table contains data for the entire network as not just the largest component. Using just the latter, transitivity is expected to be even lower.

## Task 5

Create a graph of the degree distributions in your network. What do you observe? What do you imply from the observed degree distributions?

```{r}
# calculate the degrees for each node

df_degrees <- as.data.frame(degrees) |> rownames_to_column(var = "ID")

# set the theme for ggplot
theme_set(theme_light())

# here, we plot the lower 95% of values in a bar plot

df_degrees |> ggplot(aes(degrees)) +
  geom_bar() +
  xlab("Degrees") +
  ylab("Count") +
  scale_x_continuous(limits = c(0, quantile(df_degrees$degrees, 0.95)), breaks = breaks_extended(n = 20)) +
  labs(title = "Distribution of the lower 95th percentile of degrees",
       caption = "The mode is 2 and the max 2766.")
```

> As can be seen, the degree distribution is heavily right-tailed, with a mode and at the same time minimum value of 2. At the same time, a maximum value of 2766 clearly indicates how unequal the degrees are distributed and how long the right tail goes. In order to preserve the interpretability of the visualization, only the nodes are shown, which correspond to the 95% lowest degree values. It is important to take this into account as the long right tail indicates many powerful hubs with 32 degree's above 1000 and 126 over 500. These hubs serve as center points for this network and likely facilitate the flow of information (emails) throughout it.

## Task 6

Create a formatted table that contains distribution of node-level centrality values.

```{r}
# eigenvector and betweenness, normalized betweenness, and  degrees for each node are calculated

df_degrees$Betweenness <- igraph::betweenness(graph, directed = FALSE)
df_degrees <- df_degrees |> mutate(betweenness_normalized = (Betweenness - min(Betweenness)) / (max(Betweenness)-min(Betweenness)))
df_degrees$Eigenvector <- eigen_centrality(graph, directed = FALSE)$vector

# here we construct a descriptive table showing the previously calculated centrality values 

datasummary((Degrees = degrees) + Betweenness + (`Betweenness (normalized)` = betweenness_normalized) + Eigenvector + 1 ~ Mean + Median + SD + N + Max + Min + Percent(),
            data = df_degrees,
            title = "Node-level centrality measures: Descriptives")
```

> As the degree distribution has been discussed at length above, betweenness and eigenvectors will be deliberated upon here. Betweenness by itself is hard to interpret, as it is not a normalized measured. Regardless of this, it should be noted that the median and minimum value is 0 with the mean value near 50 thousand. As the mean is much more affected by extreme outliers, such as the maximum value in the millions, the very high standard deviation makes sense. Additionally, this indicates that there are a few nodes with high betweenness values, meaning that their ties enable many paths between nodes to pass through. To increase interpretability, a normalized betweenness has been computed.

> Eigenvector centrality takes into account the centrality of each node's neighbouring nodes to identify highly influential nodes within networks. Within the Enron Email network, most nodes have very low eigenvector values, as the mean and median are both near 0. The maximum value is at 1, showing that there are at least some node which are characterized by highly influential neighbouring nodes.

## Task 7

Considering the large network size, select a form of visualization that could be meaningful. Visualize your network and color the nodes according to a selected actor-variable or according to a selected measure of centrality.

```{r}
#| echo: false
#| include: false

# here I add the to graph object different centrality measures in case I want to use them in the visualization
V(graph)$degrees <- df_degrees |> select(degrees) |> pull(degrees)
V(graph)$between <- df_degrees |> select(Betweenness) |> pull(Betweenness)
V(graph)$between_normal <- df_degrees |> select(betweenness_normalized) |> pull(betweenness_normalized)
V(graph)$eigen <- df_degrees |> select(Eigenvector) |> pull(Eigenvector)


# from here I work on creating a sub_graph that will make it easier to visualized the results

# I create a data frame that for each node tells me how often it occurs in the initial data set
counts <- data %>%
  pivot_longer(cols = everything()) %>%
  group_by(value) %>%
  summarise(count = n()) %>%
  ungroup()

# then I filter so that I only keep nodes with more than 30 occurrences and save them to a numeric vector
high_count_nodes <- counts %>%
  filter(count >= 30) %>%
  pull(value)

# I use these high_count_nodes to filter the original edge list to only contain those nodes which occur in high_count_nodes
sub_data <- data |> filter(FromNodeID %in% high_count_nodes & ToNodeID %in% high_count_nodes)

# I construct a new graph from these subset data
subgraph_new2 <- graph_from_data_frame(sub_data, directed = FALSE)

# here I check the components with the goal of removing them
component_list_subgraph <- igraph::components(subgraph_new2)

# get the largest component
largest_component_subgraph <- which.max(component_list_subgraph$csize)

# extract  nodes of the largest component
vertices_largest_component_subgraph <- which(component_list_subgraph$membership == largest_component_subgraph)

# create subgraph containing only the nodes of the largest component
largest_subgraph2 <- induced_subgraph(subgraph_new2, vertices_largest_component_subgraph)

# sanity check if I only have one component left (I do)
igraph::components(largest_subgraph2)

# calculate eigenvector centrality for all nodes in this graph object
eigen_new <- eigen_centrality(largest_subgraph2, directed = FALSE)$vector

# ... and attach them to the vertices
V(largest_subgraph2)$eigen_new <- eigen_new

# here I create a function that helps me normalize values
# this I then use to split the eigenvector centralities into 10 categories (0.1, 0.2, etc.) for plotting later
normalize <- function(x){(x-min(x))/(max(x)-min(x))}
(V(largest_subgraph2)$ec_index <- round(normalize(V(largest_subgraph2)$eigen_new) * 9) + 1)

# here I add color information to the vertices from a color gradient so that depending on the corresponsing value on the 10 categories of eigenvector centrality a different color will be shown when I plot it
V(largest_subgraph2)$color <- colorRampPalette(c("turquoise", "yellow","red"))(10)[V(largest_subgraph2)$ec_index]
```

```{r}
# here I plot this subgraph, making the nodes with high eigenvectors bigger and differ in color (based on categorical variable just created)
subgraph_test_new <- plot.igraph(largest_subgraph2,
            vertex.label = "",
            vertex.size = 2+3*V(largest_subgraph2)$ec_index,
            main = "Network of Enron emails: Eigenvector sized & colored")
```

> The following graph presents a greatly subset version of the network, in which all but the major component are removed and only vertices with more than 30 occurances in the original edgelist (i.e., a degree of 15, as this is an undirected data set with ties always being recorded "in both directions", e.g., "1-0" & "0-1") are removed.

> In 10 equidistant steps from \>0.1, to 1 eigenvector centrality designates a color for each node. This results in the majority of nodes being light blue, with the values around 0.5 being yellow and the most extreme outliers being colored red. Depending on their eigenvector centrality, they are also adjusted in size, so as to be visible among all the low eigenvector centrality nodes.

> Looking at the graph it is (likely, as it varies from execution to execution of code) visible that there are several hubs of prestige around the center of the graph with smaller hubs at the edges. This could indicate that the majority of the corporation is fairly closely connected but that some teams may be less involved in the day-to-day at the corporation. Such teams could be workers, who are mainly responsible in the upkeep of buildings or servers, etc., while the teams tasked with the "substantive" side of business are more entwined and central in the graph. Importantly, each cluster of nodes seems to have at least one node that has much higher eigenvector values than the rest. These are likely individuals in managerial positions who not only hold contact with each member of their team but also coordinate actions with their equals (managers of other teams) and their superiors,

## Task 8

Check assortativity in the network by centrality measures. What do you observe? Provide an interpretation of your findings. What kind of theoretical arguments could possibly explain your results?

```{r}

# calculation of assortativity scores for three main centrality measures
assortativity_eigen <- assortativity(graph, V(graph)$eigen)
assortativity_between <- assortativity(graph, V(graph)$between)
assortativity_degrees <- assortativity_degree(graph, directed = FALSE)

cat(paste0("The assortativity value for eigenvector centrality is ",assortativity_eigen,"."))
cat(paste0("The assortativity value for betweenness centrality is ",assortativity_between,"."))
cat(paste0("The assortativity value for degree centrality is ",assortativity_degrees,"."))
```

> Assortativity describes the assortative mixing of of certain characteristics within a given network (Kolaczyk & CsÃ¡rdi, 2014). Similarly to the Pearson correlation coefficient, it provides a value between -1 and 1, indiciating absolute negative and positive relation, respectively and 0 being no correlation at all. In the case of assortativity, a value near 0, as seen in all three assortativity scores calculated for this network, shows that in general the observed nodes show no greater tendency to be tied to nodes that carry similar centrality measures (or dissimilar in the case of a negative correlation).

> Within the email network of a large corporation, these results make sense. It is fairly likely that individuals in managerial positions have high centrality scores but have many ties to other high centrality managers but also their immediate subordinates. These subordinates vastly outnumber them but perhaps have more "upward" than "sideward" communication within the organizational hierarchy. Another reason for these scores may be the many components which are taken into account within this calculation. As they are not connected to the main component, they are expected to have high assortativity correlations, thus perhaps balancing out the rest of the network to an extent.

## Task 9

And finally, a thought exercise. Assume that the robustness or vulnerability of the network is examined. Try to come up with a measurement of robustness / vulnerability and argue for the usefulness of your measure. Speculate about some implications for the concrete network.

> One such measurement for vulernability could be how many of the highest scoring nodes in betweenness centrality one would have to remove for the network to break apart into a predefined k components. This would have the advantage of integrating information gained from observing the clustering within a previously run plot to inform the value of k. Though this measure would not be very comparable acorss networks, within the same network it could give an indication of varying strenght of connections between components.

> In the case of an email network I suspect that "elminating" managerial positions (which I presume are very high in betweenness) might quite quickly break the network into more components. This might be a faulty assumption though, as it is also feasible that the most central nodes in this network could simply be accounts with automated messages (e.g., regularily scheduled reminders by HR to enter your working hours, etc.). Therefore, this measure would have to take into account the actors' roles in their network too, to make sense substantively.
